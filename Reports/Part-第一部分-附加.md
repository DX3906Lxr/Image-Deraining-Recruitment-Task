# 1.我读的两篇论文
## 论文一：Progressive Image Deraining Networks: A Better and Simpler Baseline
PRN网络本身确实很简洁      
我们先来讨论它最基本的“渐进式”结构，然后再来将什么为什么要每次cat y，以及在进入Resblosks之前加一个ReLu激活的好处，以及LSTM.      
### 对比我们之前实现的baseline的网络
在进入Resblocks之前之后也都进行了卷积操作，来分别进行低层特征提取和把深层特征映射回像素域，得到图像形式的输出
这是很好理解的，baseline采用的是多个残差块的直接堆叠，      
而PRN是反复进入同一个残差块    
对于baseline这种，很好理解，就是先学局部特征，然后逐步学全局特    
对于PRN的更具体的逻辑到底是怎样的，论文中也没说     
我仔细研究发现，还真有不少有趣的地方
如图所示，我觉得PRN和baseline还是有本质区别的      
![alt text](<img/截屏2025-10-15 21.29.25.png>)
同样的“多阶段”     
baseline的用深的网络对特征在各个阶段，进行逐步学习，所以最后才能用这些学到的特征，靠最后的卷积“合成”我们想要的东西       
而PRN是靠对“预测图”的在多阶段逐步“去雨”，最终合成预测图        
baseline的多阶段是特征通道之间的多阶段即，是对那64个特征通道的多个阶段
而PRN的多阶段，直接是对预测图像（当然，在本网络中，既可以是预测“雨层”，也可以直接预测“去雨图”）的多个阶段
即，每个“阶段”其实都可以看作是一个“端到端”的模块     
这样就比较清晰了      
再具体一点，比如baseline各个阶段的ResBlock各个卷积负责利用前面学到的局部特征再学习出新的特征，最后就是可以简单地看作每个卷积核负责一个特征或语义
### 那PRN的ReslBlock卷积在干嘛呢？   
论文的原话是“extract deep representation”       
好像就是baseline的整个ResBlock干的事       
>不过PRN的ResBlock卷积层很短呀，它真的能提取到深处表征吗？或者说全局特征吗？      

而且PRN是要反复用同一套参数（所以的卷积参数，包括ResBlock前后的卷积）反复”去雨“，对于不同的”输入图片“，在不同阶段他要用同一套参数去干不同的事
比如第1阶段在去掉大雨丝，第3阶段又修复细节      
这其实有点“反直觉”，毕竟参数都没变，居然能这么“灵活”，就好像有自适应能力一样
我还是想从正向和反向传播两方面来理解
#### 正向传播     
从最终的效果上看，确实是同一套参数干出了这些不同的变化      
就是说虽然我单独的一个阶段的特征提取不如深处卷积的好，但是好在我可以“处理”很多次      
也就是说一套参数不必像baseline的深处卷积一样，一个卷积核处理一次就到达到它的目的，而是拥有在不同的阶段完成不同的事的能力     
也就是baseline的卷积核是一个专攻某个方面的专家（它只用它的那套参数干一类事（这或许也是这个网络容易过拟合的原因？🤔））      
而PRN的卷积核是一个各方面都有涉及的通用人才（它在不同方面干的肯定没在baseline的对应功能的卷积好，毕竟人家还要干其他事，但胜在它很全面，可以反复利用它🥵）     
嘿嘿，这个比喻我认为是相当形象和本质的（有点小得意了😄）     
#### 反向传播     
卷积层里的参数是同一个tensor（同一个内存地址）      
反向传播时，这个tensor的.grad 会自动累积所有阶段的梯度和     
也就是说：      
PRN里的共享参数的梯度等于来自所有阶段反向传播梯度的总和     
也就是说其实每次求梯度，是将各个阶段的效果考虑都进来的      
也就是说参数会根据所有阶段的误差信号"共同"调整      
最终各阶段的误差都很小，都用同一套参数完成了各自的任务      
### 和原图拼接      
这个操作的原因不难理解       
重点就在于每个阶段都是一个端到端的过程    
就是说要对图片“修改”很多次       
而不像baseline里面的，baseline其实只有最后是在“合成”或者说“修改”图像，前面的都是针对“特征”的处理     
PRN要对图片“修改”很多次这就有一个有趣的问题了      
我改了图片后，我怎么知道原图长啥样啊？        
比如，对于一个输入的上阶段的图，在这个阶段我怎么知道哪些地方是“残留雨丝”      
哪些地方是“背景但被误修了”      
我并没有一个参考     
当然了，最后的损失函数当然是一个参考，但那“太远了”      
所以就进行了与原图的拼接，网络能在特征变换阶段直接看到原始像素信息      
对去雨、去噪、恢复细节特别有效      
我想，这是很好理解的       

### 关于baseline和PRN在head层有没有ReLu激活函数也是一个值得研究的问题      
（虽然论文中并没有把进入Resblocks前的层叫做head，但是我们姑且这样叫他）     
ReLu带来的是非线性特征，我的理解是baseline的head层仅仅是提取底层特征，对特征的具体处理是在Resblocks里面完成的
也就是说如果“提前”非特征，那可能就会提前破坏局部特征了
当然了本身Resblocks里面也有ReLu，但也就是残差网络的特性，那不是“强制性”的      
而如果在head层“强制”人家，那就不好了      

而对于PRN，它并非像baseline一样，是逐步的特征提取    
它每个阶段，也就是每个Resblosk干的都是很多很复杂的事（就像前面所的“专家”与“通才”的比喻）      
它的第一个阶段的Resblocks可能就在干很复杂的非线性了，而不是像baseline一样，要等到后面的Resblocks才行    
当然前面说的破坏局部特征，应该也有影响，但是也说了PRN要的是“通才”而非“专家”，当然了拼上原图可能也恰好弥补一些“细节”的丢失      

## 论文二：Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset
>我只关注了这篇论文中关于模型的部分，数据集的处理就没有关注了  

![alt text](<img/截屏2025-10-14 15.27.52.png>)

这个网络乍一看似乎复杂不少，但似乎本质上也就仅仅是残差网络加入了所谓的注意力图而已       
而且由于论文中作者给的阐述还是比较详细，所以理解起来其实比第一篇的PRN要轻松不少     
### 注意力图
ok，先来说说这个空间注意力图，这是我第一次接触“注意力图”，定义其实很简单     
当然，不止有空间注意力，还有其他注意力，如下图所示    
![alt text](<img/截屏2025-10-14 07.53.47.png>)
论文中其实全片并没有讲注意力图是什么，所以其实我如果早点查到它的定义，可能对我第一次读更好地理解这篇论文有所帮助，也算是一种经验吧，对于标题，或者文章中的高频词，如果不懂，可以尽早去查     
### 空间注意力图
回到空间注意力图     
它反映了模型在输入图像中“认为哪些区域可能是雨丝”的概率    
可以把它理解成：     
模型的“雨丝热力图”     
红色区域：模型高度怀疑是雨丝      
蓝色区域：模型认为是背景      

将这张图加入到残差网络，如图所示       
![alt text](<img/截屏2025-10-14 15.31.17.png>)

来告诉模型专注于哪里      
当然了，重点是这种图是怎么生成的，用的是空间注意模块，即SAM，然后SAM中主要用的是IRNN架构    
#### IRNN
所以我们来讲讲这个IRNN架构     
论文中下面这段话都是相当关键的      
甚至我认为是整片论文思路的核心       
>“在[3]中，采用两轮四向IRNN架构利用上下文信息提升小目标检测性能。首轮IRNN旨在生成能综合输入图像各位置邻域上下文的特征图，而次轮IRNN则进一步收集非局部上下文信息以生成全局感知特征图。近期，Hu等人[18]同样基于方向性对识别阴影/非阴影区域具有重要作用的观察，利用该双轮四向IRNN架构进行阴影区域检测。他们设计了方向感知注意力机制以生成更具区分度的上下文特征”      

就是说Hu等人在[18]中已经利用了IRNN生成全局感知特征图的特点，再引入了方向感知注意力机制      
本论文也就是采用这样的思路    
具体实现，论文中也说了       
>空间注意力模块（SAM）。我们基于前述两轮四向IRNN架构构建SAM。通过IRNN模型将雨痕投影至四个主方向。另一分支用于捕捉空间上下文信息，从而选择性地突出投影的降雨特征，如图5（d）所示      
与［181在嵌入空间中隐式学习方向感知特征不同，我们通过显式监督进一步采用额外的卷积和sigmoid激活，     
明确生成注意力图。注意力图揭示降雨空间分布特征，并引导后续去雨处理流程      

普通RNN，它沿时间t传播，每一步依赖前一个隐藏状态（即ht-1）       
也就是是使用潜变量来记录过去是信息      
而以它为基础的的IRNN，也是这个思想，它不再“遗忘”信息
![alt text](<img/截屏2025-10-14 11.44.02.png>) 
![alt text](<img/截屏2025-10-14 11.53.07.png>)
也就像论文中的说的      
>“可以非常稳定地在很长距离（或很大空间）上传播特征”

将 RNN 从“时间方向”变成“空间方向”     
在图像任务中，我们可以把图像的行列看作“时序维度”：    
从左到右传播：每个像素接收它左边像素的信息；  
从上到下传播：每个像素接收它上方像素的信息；   
从右到左、从下到上同理     

下图是一个更具体的呈现    
![alt text](<img/截屏2025-10-14 12.36.08.png>)    
这就是论文中“投影”的含义，确实挺形象，就像光透过一层层“幻灯片”，使得该方向每个“像素”的信息都能有所保留    

第一轮：生成局部邻域的方向特征    
第二轮：再沿相同四方向传播一次，使每个像素能“看到”整张图的全局方向分布    

![alt text](<img/截屏2025-10-14 12.56.35.png>) 
上图就是两轮投影的形象体现（只针对一个像素）    
为什么两轮就能“使每个像素能“看到”整张图的全局方向分布”
其实并不是那么“显然”      
我是这样理解的，就如上图所示，第一轮是每个像素沿着四个方向“投影”，可以说是A像素的信息传递给了以它为中心的四个方向上的每一个像素，如中间的图所示     
然后中间图中这些带着A像素的信息，传递到整个图，也就是每个像素的信息，其实都是在整个图上进行了传递     
这当然也就等效于“使每个像素能“看到”整张图的信息”也就是”整张图的全局方向分布”     

最终输出的四个方向特征被拼接（concatenate）后，再经过卷积融合，得到综合的空间表示    
这个是大体流程     

##### 具体公式
然后再来讲讲具体的公式      
![alt text](<img/截屏2025-10-14 11.44.02.png>) 
![alt text](<img/截屏2025-10-14 10.56.46.png>)
第二个感觉像写错了，其实不然       
因为在代码实现中，IRNN通常先用      
h（i,j）存储当前像素的输入特征（即 X（i,j））     
所以 h（i,j）在更新前其实代表X（i,j）     
然后还有一点不同，就是“输入”的前面有没有权重     
这是源于论文的     
>"采用ReLU激活函数和单位矩阵初始化的递归神经网络     （IRNN）¹¹用于自然语言处理[23]，已被证明具有易于训练、擅长建模长程依赖关系以及高效等特点"     

这里的单位矩阵初始化就是为什么输入的前面没有权重     
当然，论文也没说为什么这样效果就好     
我问了下AI，简单就是因为     
让梯度稳定传播     
以及输入特征已经被卷积层提取     
在SPANet的SAM模块中，IRNN 的输入X并不是原始像素，而是卷积层输出的特征图     
卷积层本身已经执行了线性变换（相当于一个W）     
     
然后潜变量前面的“方向权重”     
就是论文中的     
>另一分支用于捕捉空间上下文信息，从而选择性地突出投影的降雨特征     
“方向权重”就是根据这个分支计算来的     

方向权重本质作用就是     
在不同的空间位置上，动态地调节四个方向特征的贡献度     
也就是方向权重告诉模型：在哪个方向上，连续结构更像雨丝，就增强该方向的特征响应     
也可以说它决定是否延续前面的信息，是忘记，还是记住     



卷积融合后，再进过Sigmoid激活     
将结果映射到 [0,1]，生成最终的Attention Map（注意力图）！     
![alt text](<img/截屏2025-10-14 15.11.07.png>)
它是一张二维图（H×W），     
其中每个像素的数值表示模型认为“这个位置属于雨丝的概率”
如上图所示     
红色表示高概率，模型判断“这地方像雨丝”     
蓝色表示低概率，模型认为“这是背景”     

而与普通的attention map不同的是：     
这个“概率判断”不是仅靠局部像素，而是基于方向一致性     
也就是说：     
它会认为 “如果一大片区域内的纹理在同一方向上连续延伸、形状细长”，那么这些区域大概率是雨丝     

然后这个注意力图被用于残差网络，于是就有了SpaNet！     
### 总体的流程就是     
>"提出的SPANet架构（a）。其采用三个标准残差块（RBs）[16]提取特征，四个空间注意力块（SABs）分四个阶段逐步识别雨痕，并通过两个残差块重建干净背景"     
![alt text](<img/截屏2025-10-14 15.41.26.png>)     


# 2.论文调研
我知道的渠道就是arxic和知网     
因为其实只读了给的前两篇论文，还没怎么调研     
所以我让AI帮我总结了     
![alt text](<img/截屏2025-10-15 12.47.57.png>) 
![alt text](<img/截屏2025-10-15 12.48.23.png>) 
![alt text](<img/截屏2025-10-15 12.48.09.png>)

# 3.论文处理     
因为其实我就处理了两篇文章，也谈不上有什么很多的经验，我就讲讲我怎么处理这两篇的     
我首先会给Ai，让他帮我总结一下     
然后把论文给“沉浸式翻译”这个软件，让他给我整体翻译     
看一下摘要，然后重点看第一部分介绍中的“contributions”
然后Related之类的就简单看看     
重点看模型的具体实现，这些地方需要更精准的翻译，我每一小段单独给deepl翻译     
重点结合他给的模型的结构图理解，了解了大概     
就可以结合模型的原代码在更进一步理解，如果不看源代码，里面很多细节论文中也没讲     
遇到理解不了的就问ai     

# 4.渐进式训练
## 这是个啥？
![alt text](<img/截屏2025-10-15 14.37.26.png>)
其中的损失项损失，其实已经在PINN解偏微分方程的时候用到了
当时如果三个损失一起训练，那么PDE损失怎么也下不去     
然后采用了先训练初始值损失的策略     
首先它本身确实比其他两项损失难收敛，不过更重要的是先满足初始条件可以为满足PDE条件提供了基础，可以说PDE损失是建立在初始损失之上的     
所以这个时候渐进式学习就发挥巨大作用了     
## 为什么现在的研究都采用这种范式
原因主要有三点：

- 优化更稳定，收敛更快
一开始就让模型面对复杂目标容易“梯度震荡”或“梯度爆炸”；而逐步引入难度可以帮助模型先学到基础特征（如边缘、纹理），再慢慢学到语义层特征

- 防止陷入局部最优
直接训练全目标模型，容易在初期被错误的梯度方向牵引；而渐进式训练让模型的参数在合理轨迹上演化

- 模仿人类学习方式
人学习也是“从易到难”，深度模型在复杂场景下也表现出相似规律

在图像去雨、超分辨率、风格迁移、PINN 等任务中，这种策略被验证能显著提升稳定性与最终性能

## 它与“出题人给出的训练方法”的区别
aster学长😊给的方法是“一步到位式训练”——直接联合优化全部loss或整个网络     
而渐进式训练分阶段逐步引入

## 它的优劣在哪？
优点：
- 稳定性更好
- 泛化性能提升
- 更容易达到高质量结果
对复杂网络（多分支、多约束）更有效

缺点：
- 实现复杂
- 训练时间长
- 需要人为设计阶段划分
- 有时会“过度依赖阶段划分”，导致泛化性下降

## 科研实践中：是否应当“遵循范式”？
当任务复杂、loss冲突明显时，可以采用这种思路
但不要盲从，要能从任务本质判断是否需要
最重要的是：提出“为什么需要渐进式”，而不是简单“用了渐进式”     
要能基于任务特点（如梯度不平衡、loss震荡、任务分层）设计自己的渐进方案
# 5.搭建属于自己的模型：
## 代码与运行结果截图
![alt text](<img/截屏2025-10-15 21.47.07.png>) 
![alt text](<img/截屏2025-10-15 21.47.14.png>) 
![alt text](<img/截屏2025-10-15 21.47.20.png>) 
![alt text](<img/截屏2025-10-15 21.47.26.png>) 
![alt text](<img/截屏2025-10-15 21.47.32.png>) 
![alt text](<img/截屏2025-10-15 21.47.39.png>) 
![alt text](<img/截屏2025-10-15 21.47.44.png>) 
![alt text](<img/截屏2025-10-15 21.47.49.png>) 
![alt text](<img/截屏2025-10-15 21.47.52.png>)
## 运行结果
呜呜，十分抱歉aster学长，我这电脑跑这个代码真的有点慢运行，所以就只运行了38轮
![alt text](<img/截屏2025-10-15 21.15.35.png>) 
![alt text](<img/截屏2025-10-15 21.15.42.png>) 
![alt text](<img/截屏2025-10-15 21.15.59.png>)
## onnx
![alt text](<img/截屏2025-10-15 23.59.31.png>) 
![alt text](<img/截屏2025-10-16 00.07.28.png>)
## 降低点数？
论文中的结构“好”，通常是和整个系统配合“好”    
单独抽离某个模块放入自己的模型，上下文（数据分布、损失配比、优化策略）一变，它的行为就会变得完全不同     
很多论文模块（如 Attention、IRNN、Transformer 分支）依赖非常细致的特征尺度平衡     
当你在别的网络中嵌入时：
通道数不一样；
BN/IN 层顺序不同；
输入特征统计分布变了；
这些都会导致 梯度放大 / 缩小 / 爆炸 / 消失，最终表现为指标下